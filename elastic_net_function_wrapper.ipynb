{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Follow [Friedman et al 2007 , pg 6](http://arxiv.org/pdf/0708.1485.pdf)\n",
    "\n",
    "$$min_{\\beta} \\frac{1}{2} \\sum (y_i -\\sum x_{ij} \\beta_j)^2 + \\lambda \\sum_{j} (\\alpha |\\beta_j| + (1-\\alpha) \\frac{\\beta_{j}^2}{2} )$$\n",
    "\n",
    "If the data are standardized so that $ \\frac{\\sum x_i^2}{n} =1 $ and $\\sum x_i=0$, the algorithm can be defined as:\n",
    "\n",
    "$$ \\beta_j \\leftarrow \\frac{S(\\frac{\\sum_i x_{ij} (y_i - \\hat{y_i^j})_{+}}{n} , \\lambda \\alpha )}{1+(\\lambda(1-\\alpha) )} $$\n",
    "\n",
    "Where the soft threshold operator is given by\n",
    "\n",
    "$$S(\\beta, \\tau) = sign(\\beta) (|\\beta|-\\tau)$$\n",
    "\n",
    "And $\\hat{y_i^j} = \\sum_{k \\neq j } x_{ik} \\beta_{k}$ stands for the fitted values of the standing betas ignoring the jth column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as sm\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Code the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as sm\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code the soft threshold operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def S(z, gamma):\n",
    "    if np.abs(z) - gamma > 0:\n",
    "        return np.sign(z)*(np.abs(z) - gamma)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code coordinate descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def coordinate_descent(data_X, data_y, b0 , maxiter = 200000, tol = 0.00001, alpha = .5, l1_ratio = 1):\n",
    "    '''\n",
    "    INPUT\n",
    "    -data_X: array of exogenous variables\n",
    "    -data_y: array (vector) of endogenous variables\n",
    "    - b0 starting point\n",
    "    -alpha: penalization/shrinking parameters\n",
    "    -l1_ratio: weight put on the lasso, 1-l1_ratio is the weight put on the ridge\n",
    "    -maxiter: maximum number of iterations\n",
    "    -tol: level of tolerance for convergence.\n",
    "    OUTPUT: path, b\n",
    "    -path: path of optimization\n",
    "    -b: optimal coefficients\n",
    "    -Note: output is for standardize coeficients\n",
    "    '''\n",
    "    #1.Standardize the data#\n",
    "    X = scale(data_X, axis=0)\n",
    "    y =scale(data_y)\n",
    "    print(X.shape)\n",
    "    #2.Initialize#\n",
    "    b = b0\n",
    "    p = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    b_new = b + np.ones(p)*3000\n",
    "    path = []; path.append(b.copy())\n",
    "\n",
    "    #3.Start outer loop!\n",
    "    for itr in range(maxiter):\n",
    "        #3.1Check convergence\n",
    "        if np.linalg.norm(b_new-b)/np.linalg.norm(b) <tol:\n",
    "            b = b_new.copy()\n",
    "            path.append(b.copy())\n",
    "            #Plot the path\n",
    "            plt.plot(path[:10])\n",
    "            #Print message:\n",
    "            print('Convergence achieved after', itr, 'iterations and coefficients',b )\n",
    "            #Return the path and the coefficients\n",
    "            return  b\n",
    "            \n",
    "        #3.2 If not convergece, take another internal loop\n",
    "        else:\n",
    "            b = b_new.copy()\n",
    "            path.append(b.copy())\n",
    "            #Loop accross coordinates\n",
    "            for j in range(p):\n",
    "                b_new[j] = S(1/N * np.dot(X[:,j], (y - (np.dot(X,b) - np.dot(X[:,j], b_new[j]))))\n",
    "                             , alpha*l1_ratio)/(1+alpha*(1-l1_ratio))\n",
    "\n",
    "        #If convergence is not achieve:\n",
    "    print('Maximum iterations', itr,'exhausted, coefficents \\n',b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "@jit( cache=True)\n",
    "def numba_coordinate_descent(data_X, data_y, b0 , maxiter = 200000, tol = 0.00001, alpha = .5, l1_ratio = 1):\n",
    "    '''\n",
    "    INPUT\n",
    "    -data_X: array of exogenous variables\n",
    "    -data_y: array (vector) of endogenous variables\n",
    "    - b0 starting point\n",
    "    -alpha: penalization/shrinking parameters\n",
    "    -l1_ratio: weight put on the lasso, 1-l1_ratio is the weight put on the ridge\n",
    "    -maxiter: maximum number of iterations\n",
    "    -tol: level of tolerance for convergence.\n",
    "    OUTPUT: path, b\n",
    "    -path: path of optimization\n",
    "    -b: optimal coefficients\n",
    "    -Note: output is for standardize coeficients\n",
    "    '''\n",
    "    #1.Standardize the data#\n",
    "    X = scale(data_X, axis=0)\n",
    "    y =scale(data_y)\n",
    "    print(X.shape)\n",
    "    #2.Initialize#\n",
    "    b = b0\n",
    "    p = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    b_new = b + np.ones(p)*3000\n",
    "    path = []; path.append(b.copy())\n",
    "\n",
    "    #3.Start outer loop!\n",
    "    for itr in range(maxiter):\n",
    "        #3.1Check convergence\n",
    "        if np.linalg.norm(b_new-b)/np.linalg.norm(b) <tol:\n",
    "            b = b_new.copy()\n",
    "            path.append(b.copy())\n",
    "            #Plot the path\n",
    "            plt.plot(path[:10])\n",
    "            #Print message:\n",
    "            print('Convergence achieved after', itr, 'iterations and coefficients',b )\n",
    "            #Return the path and the coefficients\n",
    "            return  b\n",
    "            \n",
    "        #3.2 If not convergece, take another internal loop\n",
    "        else:\n",
    "            b = b_new.copy()\n",
    "            path.append(b.copy())\n",
    "            #Loop accross coordinates\n",
    "            for j in range(p):\n",
    "                b_new[j] = S(1/N * np.dot(X[:,j], (y - (np.dot(X,b) - np.dot(X[:,j], b_new[j]))))\n",
    "                             , alpha*l1_ratio)/(1+alpha*(1-l1_ratio))\n",
    "\n",
    "        #If convergence is not achieve:\n",
    "    print('Maximum iterations', itr,'exhausted, coefficents \\n',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proceed to test**\n",
    "\n",
    "#### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "pp = 100\n",
    "n = 100000\n",
    "n_samples, n_features =n , pp\n",
    "data_X = np.random.randn(n_samples, n_features)\n",
    "coef = 3 * np.random.randn(n_features)\n",
    "inds = np.arange(n_features)\n",
    "np.random.shuffle(inds)\n",
    "coef[inds[5:]] = 0  # sparsify coef\n",
    "data_y = np.dot(data_X, coef)\n",
    "# add noise\n",
    "data_y += 0.02 * np.random.normal((n_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the elastic net and the two versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "data_X =scale(data_X, axis = 0)\n",
    "data_y = scale(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "enet = ElasticNet(alpha=1, l1_ratio=0.5).fit(data_X,data_y).coef_\n",
    "enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "coordinate_descent(data_X, data_y, b0 = np.zeros(data_X.shape[1]), maxiter = 200000, tol = .1, alpha = 1, l1_ratio = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "numba_coordinate_descent(data_X, data_y, b0 = np.zeros(data_X.shape[1]), maxiter = 200000, tol = 0.1, alpha = 1, l1_ratio = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
