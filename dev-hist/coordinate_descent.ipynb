{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow [Friedman et al 2007 , pg 6](http://arxiv.org/pdf/0708.1485.pdf)\n",
    "\n",
    "$$min_{\\beta} \\frac{1}{2} \\sum (y_i -\\sum x_{ij} \\beta_j)^2 + \\lambda \\sum_{j} (\\alpha |\\beta_j| + (1-\\alpha) \\frac{\\beta_{j}^2}{2} )$$\n",
    "\n",
    "If the data are standardized so that $ \\frac{\\sum x_i^2}{n} =1 $ and $\\sum x_i=0$, the algorithm can be defined as:\n",
    "\n",
    "$$ \\beta_j \\leftarrow \\frac{S(\\frac{\\sum_i x_{ij} (y_i - \\hat{y_i^j})_{+}}{n} , \\lambda \\alpha )}{1+(\\lambda(1-\\alpha) )} $$\n",
    "\n",
    "Where the soft threshold operator is given by\n",
    "\n",
    "$$S(\\beta, \\tau) = sign(\\beta) (|\\beta|-\\tau)$$\n",
    "\n",
    "And $\\hat{y_i^j} = \\sum_{k \\neq j } x_{ik} \\beta_{k}$ stands for the fitted values of the standing betas ignoring the jth column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fElastic_net(x,y,alpha, l1_ratio, maxiter, tol):\n",
    "    '''\n",
    "    -x: array of exogenous variables\n",
    "    -y: array (vector) of endogenous variables\n",
    "    -alpha: penalization/shrinking parameters\n",
    "    -l1_ratio: weight put on the lasso, 1-l1_ratio is the weight put on the ridge\n",
    "    -maxiter: maximum number of iterations\n",
    "    -tol: level of tolerance for convergence.\n",
    "    '''\n",
    "\n",
    "    #1. Standardize the data#\n",
    "    x =sklearn.preprocessing.scale(x, axis = 0)\n",
    "    y = sklearn.preprocessing.scale(y)\n",
    "\n",
    "    #2. Retrieve parameter of interest#\n",
    "    #Sample size\n",
    "    n = x.shape[0]\n",
    "    #Number predictors\n",
    "    k = x.shape[1]\n",
    "\n",
    "    #3.Initialize the beta vector#\n",
    "    betas = np.ones(k)\n",
    "    betas_last = betas +10\n",
    "\n",
    "    #4. Start outer loop\n",
    "    for it in range(maxiter):\n",
    "        #Check convergence#\n",
    "        #Depends on proportional increase\n",
    "        if np.linalg.norm(betas_last-betas)/np.linalg.norm(betas_last) <tol:\n",
    "            print('converged  at iteration %d'%it,'with difference %d'%np.linalg.norm(betas_last-betas), 'and betas',betas)\n",
    "            return betas\n",
    "            break;     \n",
    "        else:#o.w. loop over coefficients\n",
    "            betas_last =np.array(betas, copy= True)\n",
    "\n",
    "            #Cycle around coordinates\n",
    "            for j in range(k):\n",
    "                #a.Calculate partial residuals#\n",
    "                #Use a mask to extract all but the j column\n",
    "                mask = np.ones(k,dtype=bool)\n",
    "                mask[[j]]= False\n",
    "                #Compute the partial residuals\n",
    "                res_j = y - np.dot(x[:,mask],betas[mask])\n",
    "                \n",
    "                #b.Regress on partial residuals residuals and obtain standard beta_ols_j#\n",
    "                beta_ols_j = np.dot(x[:,j],res_j)/n\n",
    "                \n",
    "                #c. Update using the soft threshold operator and adjust for ridge#\n",
    "                betas[j] = (np.sign(beta_ols_j)*(np.abs(beta_ols_j)-alpha*l1_ratio))/(1+alpha*(1-l1_ratio))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the OLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged  at iteration 482 with difference 0 and betas [-0.00618541 -0.14813387  0.32109297  0.20037221 -0.48966253  0.29474712\n",
      "  0.06256708  0.10941377  0.46417943  0.04177067]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00618541, -0.14813387,  0.32109297,  0.20037221, -0.48966253,\n",
       "        0.29474712,  0.06256708,  0.10941377,  0.46417943,  0.04177067])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data to play with\n",
    "diabetes = datasets.load_diabetes()\n",
    "x = diabetes.data\n",
    "y = diabetes.target\n",
    "#Setup arguments\n",
    "#Regularization parameters\n",
    "alpha = 0#similar to OLS\n",
    "#Convex combination\n",
    "l1_ratio = 1\n",
    "#Maximum iterations\n",
    "maxiter = 200000\n",
    "#Tolerance\n",
    "tol = 1e-5\n",
    "\n",
    "fElastic_net(x,y,alpha, l1_ratio, maxiter, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.518\n",
      "Model:                            OLS   Adj. R-squared:                  0.507\n",
      "Method:                 Least Squares   F-statistic:                     46.38\n",
      "Date:                Mon, 11 Apr 2016   Prob (F-statistic):           2.68e-62\n",
      "Time:                        00:55:37   Log-Likelihood:                -466.00\n",
      "No. Observations:                 442   AIC:                             952.0\n",
      "Df Residuals:                     432   BIC:                             992.9\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0062      0.037     -0.168      0.867        -0.079     0.066\n",
      "x2            -0.1481      0.038     -3.922      0.000        -0.222    -0.074\n",
      "x3             0.3211      0.041      7.822      0.000         0.240     0.402\n",
      "x4             0.2004      0.040      4.964      0.000         0.121     0.280\n",
      "x5            -0.4893      0.257     -1.903      0.058        -0.995     0.016\n",
      "x6             0.2945      0.209      1.408      0.160        -0.117     0.706\n",
      "x7             0.0624      0.131      0.476      0.634        -0.195     0.320\n",
      "x8             0.1094      0.100      1.098      0.273        -0.086     0.305\n",
      "x9             0.4641      0.106      4.375      0.000         0.256     0.673\n",
      "x10            0.0418      0.041      1.026      0.305        -0.038     0.122\n",
      "==============================================================================\n",
      "Omnibus:                        1.506   Durbin-Watson:                   2.029\n",
      "Prob(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\n",
      "Skew:                           0.017   Prob(JB):                        0.496\n",
      "Kurtosis:                       2.726   Cond. No.                         21.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "#Standardize the data\n",
    "x =sklearn.preprocessing.scale(x, axis = 0)\n",
    "y = sklearn.preprocessing.scale(y)\n",
    "#Run the model\n",
    "mod = sm.OLS( y,x)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Compare with the elastic net package for the regularize case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged  at iteration 15 with difference 0 and betas [-0.17231757 -0.29164263  0.09726452  0.02783835 -0.18492683 -0.14766446\n",
      "  0.1045021   0.1439201   0.14110477 -0.03157433]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.17231757, -0.29164263,  0.09726452,  0.02783835, -0.18492683,\n",
       "       -0.14766446,  0.1045021 ,  0.1439201 ,  0.14110477, -0.03157433])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data to play with\n",
    "diabetes = datasets.load_diabetes()\n",
    "x = diabetes.data\n",
    "y = diabetes.target\n",
    "x =sklearn.preprocessing.scale(x, axis = 0)\n",
    "y = sklearn.preprocessing.scale(y)\n",
    "#Setup arguments\n",
    "#Regularization parameters\n",
    "lambda1 = 1#similar to OLS\n",
    "#Convex combination\n",
    "alpha = .5\n",
    "#Maximum iterations\n",
    "maxiter = 200000\n",
    "#Tolerance\n",
    "tol = 1e-5\n",
    "\n",
    "fElastic_net(x,y,lambda1, alpha, maxiter, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      ,  0.      ,  0.048895,  0.      ,  0.      ,  0.      ,\n",
       "       -0.      ,  0.      ,  0.029379,  0.      ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=1, l1_ratio=0.5).fit(x,y).coef_\n",
    "enet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The result is very different :(  something must be wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
